# ğŸ“š RAG Concepts Explained
## Interview-Ready Reference Guide

---

## ğŸ§© 1. Chunking vs Tokens

### What is a Token?
A **token** is the smallest unit of text that an AI model processes. It's NOT the same as a word.

| Text | Tokens | Why? |
|------|--------|------|
| "hello" | 1 token | Common word |
| "JPMorgan" | 2 tokens | "JP" + "Morgan" |
| "cryptocurrency" | 3 tokens | "crypt" + "o" + "currency" |
| "10-K" | 3 tokens | "10" + "-" + "K" |

**Rule of thumb:** 1 token â‰ˆ 0.75 words (or 4 characters)

### What is Chunking?
**Chunking** = Splitting a large document into smaller pieces (chunks)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Large 10-K Document (1.2 MB)                 â”‚
â”‚   "JPMorgan Chase & Co. is a financial holding company..."     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“ CHUNKING
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Chunk 1     â”‚ â”‚  Chunk 2     â”‚ â”‚  Chunk 3     â”‚ ... â”‚  Chunk 508   â”‚
â”‚  ~600 tokens â”‚ â”‚  ~600 tokens â”‚ â”‚  ~600 tokens â”‚     â”‚  ~600 tokens â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why Chunk?
1. **LLM context limits**: GPT-4 can only read ~128K tokens at once
2. **Precise retrieval**: Find specific relevant sections, not entire docs
3. **Cost efficiency**: Process less tokens = cheaper API calls

### Chunk Parameters
| Parameter | Value Used | Purpose |
|-----------|------------|---------|
| **Chunk Size** | 600 tokens | Balance of context vs precision |
| **Overlap** | 100 tokens | Prevent cutting sentences mid-thought |
| **Min Size** | 100 tokens | Don't create tiny useless chunks |

### Overlap Explained
```
Chunk 1:  [==================]
Chunk 2:        [==================]  â† 100 tokens overlap with Chunk 1
Chunk 3:              [==================]
```
Overlap ensures important information at chunk boundaries isn't lost.

---

## ğŸ§  2. Embeddings & Dimensions

### What is an Embedding?
An **embedding** converts text into numbers (a vector) that captures **meaning**.

```
Text: "The bank manages credit risk"
         â†“ Embedding Model
Vector: [0.23, -0.45, 0.12, 0.89, -0.34, ...]  â† 384 numbers
```

### The Magic: Similar Meanings â†’ Similar Vectors
```
"The bank manages credit risk"     â†’ [0.23, -0.45, 0.12, ...]
"Credit risk management at banks"  â†’ [0.25, -0.43, 0.14, ...]  â† SIMILAR! (close numbers)
"I love pizza"                     â†’ [-0.82, 0.67, -0.91, ...] â† DIFFERENT! (far apart)
```

### What are Dimensions?
Each number in the vector represents one "dimension" of meaning:

| Dimension | What it might capture (simplified) |
|-----------|-----------------------------------|
| Dim 1 | Is it about finance? (+) or food? (-) |
| Dim 2 | Is it positive? (+) or negative? (-) |
| Dim 3 | Is it about risk? (+) or opportunity? (-) |
| ... | ... |
| Dim 384 | Some other learned pattern |

### Dimension Comparison
| Model | Dimensions | Quality | Speed | Cost |
|-------|------------|---------|-------|------|
| all-MiniLM-L6-v2 | 384 | Good | âš¡ Fast | Free |
| all-mpnet-base-v2 | 768 | Better | Medium | Free |
| text-embedding-3-small | 1536 | Great | Fast | $ |
| text-embedding-3-large | 3072 | Best | Slow | $$ |

**We used 384 dimensions** - best balance of speed and quality for local RAG.

---

## ğŸ—„ï¸ 3. Vector Database Explained

### What is a Vector Database?
A database optimized to store and search **vectors** (lists of numbers).

### The Problem It Solves
```
You have: 2,332 chunks, each with 384 numbers
User asks: "What is JPM's credit risk?"
Need: Find the most similar chunks FAST

Naive approach: Compare query to ALL 2,332 chunks = SLOW âŒ
Vector DB: Use smart algorithms (HNSW, IVF) = FAST âœ…
```

### How Similarity Search Works
```
User Query: "What is JPM's credit risk?"
     â†“
Convert to vector: [0.31, -0.22, 0.77, ...]
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         VECTOR DATABASE                 â”‚
â”‚   Search using cosine similarity        â”‚
â”‚   Find vectors "closest" to query       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
Top 5 most similar chunks:
1. Chunk 847 (similarity: 0.89) â† About credit risk!
2. Chunk 849 (similarity: 0.85)
3. ...
```

### Similarity Metrics
| Metric | Formula | Best For |
|--------|---------|----------|
| **Cosine** | angle between vectors | Text similarity (most common) |
| **Euclidean** | straight-line distance | When magnitude matters |
| **Dot Product** | vector multiplication | Normalized embeddings |

---

## ğŸ†š 4. ChromaDB vs FAISS vs Other Vector DBs

### Quick Comparison

| Feature | ChromaDB | FAISS | Pinecone | Weaviate | Qdrant |
|---------|----------|-------|----------|----------|--------|
| **Type** | Local/Cloud | Local only | Cloud only | Cloud/Local | Cloud/Local |
| **Setup** | 1 line | Medium | Easy | Complex | Medium |
| **Metadata** | âœ… Yes | âŒ No | âœ… Yes | âœ… Yes | âœ… Yes |
| **Persistence** | âœ… Yes | Manual | âœ… Yes | âœ… Yes | âœ… Yes |
| **Cost** | Free | Free | Paid | Freemium | Freemium |
| **Scale** | Medium | Massive | Massive | Large | Large |
| **Best For** | Prototyping, small-medium apps | Pure speed, research | Production, enterprise | Complex queries | Performance |

### Detailed Breakdown

#### ChromaDB (What we used) âœ…
```python
pip install chromadb

import chromadb
client = chromadb.PersistentClient(path="./db")
collection = client.create_collection("docs")
collection.add(documents=["text"], embeddings=[[0.1, 0.2, ...]], ids=["id1"])
```
**Pros:**
- Dead simple setup
- Built-in embedding functions
- Stores metadata with vectors
- Persists to disk
- Python-native

**Cons:**
- Not for billion-scale data
- Newer, less battle-tested

#### FAISS (Facebook AI)
```python
pip install faiss-cpu

import faiss
index = faiss.IndexFlatL2(384)  # 384 dimensions
index.add(vectors)
distances, indices = index.search(query_vector, k=5)
```
**Pros:**
- Blazing fast (C++ core)
- Handles billions of vectors
- GPU support
- Research-proven

**Cons:**
- No metadata storage
- No persistence (manual save/load)
- Steeper learning curve

#### Pinecone (Cloud)
```python
pip install pinecone-client

import pinecone
pinecone.init(api_key="key")
index = pinecone.Index("my-index")
index.upsert(vectors=[("id1", [0.1, 0.2, ...], {"key": "value"})])
```
**Pros:**
- Fully managed (no infra)
- Massive scale
- Real-time updates
- Great dashboard

**Cons:**
- Paid service
- Requires internet
- Data leaves your system

#### When to Use What?

| Scenario | Recommendation |
|----------|----------------|
| Learning/Prototyping | **ChromaDB** |
| Speed-critical research | **FAISS** |
| Production with budget | **Pinecone** or **Qdrant Cloud** |
| Self-hosted production | **Weaviate** or **Qdrant** |
| Existing Postgres | **pgvector** |

---

## ğŸ”„ 5. Complete RAG Pipeline

### What is RAG?
**R**etrieval-**A**ugmented **G**eneration = Give the LLM relevant context before asking it to answer.

### Why RAG?
| Problem | Without RAG | With RAG |
|---------|-------------|----------|
| LLM knowledge cutoff | "I don't know about 2024" | "Based on the 2024 10-K..." |
| Hallucination | Makes up facts | Cites actual documents |
| Specificity | Generic answers | Company-specific answers |
| Verifiability | "Trust me" | "Source: JPM 10-K page 47" |

### RAG Pipeline Architecture
```
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚         RAG PIPELINE            â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INDEXING (Offline - Done Once):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   10-K      â”‚ â†’â†’ â”‚   Chunk     â”‚ â†’â†’ â”‚  Embedding  â”‚ â†’â†’ â”‚  Vector     â”‚
â”‚   Documents â”‚    â”‚   (split)   â”‚    â”‚   Model     â”‚    â”‚   Database  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

QUERYING (Online - Per Question):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User      â”‚ â†’â†’ â”‚  Embedding  â”‚ â†’â†’ â”‚  Vector     â”‚ â†’â†’ â”‚   Top-K     â”‚
â”‚   Question  â”‚    â”‚   Model     â”‚    â”‚   Search    â”‚    â”‚   Chunks    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Answer    â”‚ â†â† â”‚  LLM (GPT-4/Gemini): "Based on the context:     â”‚
â”‚   + Source  â”‚    â”‚  JPM's main risks include..." [Source: ...]     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components

| Component | Our Implementation | Purpose |
|-----------|-------------------|---------|
| **Document Loader** | `A_SEC_EDGAR.py` | Download 10-K filings |
| **Chunker** | `B_Chunking_Indexing.py` | Split into 600-token chunks |
| **Embedding Model** | all-MiniLM-L6-v2 | Convert text â†’ 384D vectors |
| **Vector Store** | ChromaDB | Store and search vectors |
| **Retriever** | `C_Retrieval.py` | Find top-5 relevant chunks |
| **Generator** | `D_Generation.py` | LLM generates answer with citations |
| **API** | `api.py` (FastAPI) | REST API for queries |

### Our Numbers

| Metric | Value |
|--------|-------|
| Documents | 3 (JPM, GS, UBS 10-Ks) |
| Total Chunks | 2,332 |
| Chunk Size | 600 tokens (~450 words) |
| Vector Dimensions | 384 |
| Retrieval Time | ~38ms |
| Top-K Retrieved | 5 chunks per query |

---

## ğŸ’¡ Interview Tips

### Common Questions & Answers

**Q: Why not just use the full document?**
> A: LLMs have context limits (GPT-4: 128K tokens). A 10-K can have 300K+ tokens. Also, retrieving specific chunks is more precise and cost-effective.

**Q: Why 600 tokens per chunk?**
> A: Sweet spot between context (enough info to be useful) and precision (specific enough to match queries). Industry standard is 500-800.

**Q: Why overlap chunks?**
> A: Sentences at chunk boundaries might get cut off. 100-token overlap ensures continuity.

**Q: ChromaDB vs FAISS?**
> A: ChromaDB for ease of use + metadata. FAISS for pure speed at massive scale. We prioritized developer experience.

**Q: How do you prevent hallucination?**
> A: 1) System prompt enforces "only answer from context", 2) Citations required, 3) Confidence scoring, 4) "I don't know" responses when context insufficient.

**Q: How would you scale this?**
> A: 1) Cloud vector DB (Pinecone/Qdrant), 2) Async processing, 3) Caching frequent queries, 4) Multiple retrieval strategies (hybrid search).

---

## ğŸ“Š Quick Reference Card

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RAG QUICK REFERENCE                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  TOKEN: Smallest unit LLM processes (~0.75 words)               â”‚
â”‚  CHUNK: Document piece (~600 tokens) for retrieval              â”‚
â”‚  EMBEDDING: Text â†’ Vector (list of numbers)                     â”‚
â”‚  DIMENSION: Each number in vector (384 for our model)           â”‚
â”‚  VECTOR DB: Database optimized for similarity search            â”‚
â”‚  SIMILARITY: How "close" two vectors are (0-1 scale)            â”‚
â”‚  TOP-K: Number of chunks to retrieve (we use 5)                 â”‚
â”‚  RAG: Retrieval-Augmented Generation                            â”‚
â”‚                                                                 â”‚
â”‚  FORMULA:                                                       â”‚
â”‚  Query â†’ Embed â†’ Search VectorDB â†’ Get Chunks â†’ LLM â†’ Answer    â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

*Last updated: January 2026*
*Project: SEC EDGAR 10-K RAG Pipeline*
