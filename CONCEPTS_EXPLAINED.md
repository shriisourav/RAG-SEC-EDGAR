# ğŸ“š RAG Concepts Explained
## Interview-Ready Reference Guide

---

## ğŸ§© 1. Chunking vs Tokens

### What is a Token?
A **token** is the smallest unit of text that an AI model processes. It's NOT the same as a word.

| Text | Tokens | Why? |
|------|--------|------|
| "hello" | 1 token | Common word |
| "JPMorgan" | 2 tokens | "JP" + "Morgan" |
| "cryptocurrency" | 3 tokens | "crypt" + "o" + "currency" |
| "10-K" | 3 tokens | "10" + "-" + "K" |

**Rule of thumb:** 1 token â‰ˆ 0.75 words (or 4 characters)

### What is Chunking?
**Chunking** = Splitting a large document into smaller pieces (chunks)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Large 10-K Document (1.2 MB)                 â”‚
â”‚   "JPMorgan Chase & Co. is a financial holding company..."     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“ CHUNKING
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Chunk 1     â”‚ â”‚  Chunk 2     â”‚ â”‚  Chunk 3     â”‚ ... â”‚  Chunk 508   â”‚
â”‚  ~600 tokens â”‚ â”‚  ~600 tokens â”‚ â”‚  ~600 tokens â”‚     â”‚  ~600 tokens â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why Chunk?
1. **LLM context limits**: GPT-4 can only read ~128K tokens at once
2. **Precise retrieval**: Find specific relevant sections, not entire docs
3. **Cost efficiency**: Process less tokens = cheaper API calls

### Chunk Parameters
| Parameter | Value Used | Purpose |
|-----------|------------|---------|
| **Chunk Size** | 600 tokens | Balance of context vs precision |
| **Overlap** | 100 tokens | Prevent cutting sentences mid-thought |
| **Min Size** | 100 tokens | Don't create tiny useless chunks |

### Overlap Explained
```
Chunk 1:  [==================]
Chunk 2:        [==================]  â† 100 tokens overlap with Chunk 1
Chunk 3:              [==================]
```
Overlap ensures important information at chunk boundaries isn't lost.

---

## ğŸ§  2. Embeddings & Dimensions

### What is an Embedding?
An **embedding** converts text into numbers (a vector) that captures **meaning**.

```
Text: "The bank manages credit risk"
         â†“ Embedding Model
Vector: [0.23, -0.45, 0.12, 0.89, -0.34, ...]  â† 384 numbers
```

### The Magic: Similar Meanings â†’ Similar Vectors
```
"The bank manages credit risk"     â†’ [0.23, -0.45, 0.12, ...]
"Credit risk management at banks"  â†’ [0.25, -0.43, 0.14, ...]  â† SIMILAR! (close numbers)
"I love pizza"                     â†’ [-0.82, 0.67, -0.91, ...] â† DIFFERENT! (far apart)
```

### What are Dimensions?
Each number in the vector represents one "dimension" of meaning:

| Dimension | What it might capture (simplified) |
|-----------|-----------------------------------|
| Dim 1 | Is it about finance? (+) or food? (-) |
| Dim 2 | Is it positive? (+) or negative? (-) |
| Dim 3 | Is it about risk? (+) or opportunity? (-) |
| ... | ... |
| Dim 384 | Some other learned pattern |

### Dimension Comparison
| Model | Dimensions | Quality | Speed | Cost |
|-------|------------|---------|-------|------|
| all-MiniLM-L6-v2 | 384 | Good | âš¡ Fast | Free |
| all-mpnet-base-v2 | 768 | Better | Medium | Free |
| text-embedding-3-small | 1536 | Great | Fast | $ |
| text-embedding-3-large | 3072 | Best | Slow | $$ |

**We used 384 dimensions** - best balance of speed and quality for local RAG.

---

## ğŸ—„ï¸ 3. Vector Database Explained

### What is a Vector Database?
A database optimized to store and search **vectors** (lists of numbers).

### The Problem It Solves
```
You have: 2,332 chunks, each with 384 numbers
User asks: "What is JPM's credit risk?"
Need: Find the most similar chunks FAST

Naive approach: Compare query to ALL 2,332 chunks = SLOW âŒ
Vector DB: Use smart algorithms (HNSW, IVF) = FAST âœ…
```

### How Similarity Search Works
```
User Query: "What is JPM's credit risk?"
     â†“
Convert to vector: [0.31, -0.22, 0.77, ...]
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         VECTOR DATABASE                 â”‚
â”‚   Search using cosine similarity        â”‚
â”‚   Find vectors "closest" to query       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
Top 5 most similar chunks:
1. Chunk 847 (similarity: 0.89) â† About credit risk!
2. Chunk 849 (similarity: 0.85)
3. ...
```

### Similarity Metrics
| Metric | Formula | Best For |
|--------|---------|----------|
| **Cosine** | angle between vectors | Text similarity (most common) |
| **Euclidean** | straight-line distance | When magnitude matters |
| **Dot Product** | vector multiplication | Normalized embeddings |

---

## ğŸ†š 4. ChromaDB vs FAISS vs Other Vector DBs

### Quick Comparison

| Feature | ChromaDB | FAISS | Pinecone | Weaviate | Qdrant |
|---------|----------|-------|----------|----------|--------|
| **Type** | Local/Cloud | Local only | Cloud only | Cloud/Local | Cloud/Local |
| **Setup** | 1 line | Medium | Easy | Complex | Medium |
| **Metadata** | âœ… Yes | âŒ No | âœ… Yes | âœ… Yes | âœ… Yes |
| **Persistence** | âœ… Yes | Manual | âœ… Yes | âœ… Yes | âœ… Yes |
| **Cost** | Free | Free | Paid | Freemium | Freemium |
| **Scale** | Medium | Massive | Massive | Large | Large |
| **Best For** | Prototyping, small-medium apps | Pure speed, research | Production, enterprise | Complex queries | Performance |

### Detailed Breakdown

#### ChromaDB (What we used) âœ…
```python
pip install chromadb

import chromadb
client = chromadb.PersistentClient(path="./db")
collection = client.create_collection("docs")
collection.add(documents=["text"], embeddings=[[0.1, 0.2, ...]], ids=["id1"])
```
**Pros:**
- Dead simple setup
- Built-in embedding functions
- Stores metadata with vectors
- Persists to disk
- Python-native

**Cons:**
- Not for billion-scale data
- Newer, less battle-tested

#### FAISS (Facebook AI)
```python
pip install faiss-cpu

import faiss
index = faiss.IndexFlatL2(384)  # 384 dimensions
index.add(vectors)
distances, indices = index.search(query_vector, k=5)
```
**Pros:**
- Blazing fast (C++ core)
- Handles billions of vectors
- GPU support
- Research-proven

**Cons:**
- No metadata storage
- No persistence (manual save/load)
- Steeper learning curve

#### Pinecone (Cloud)
```python
pip install pinecone-client

import pinecone
pinecone.init(api_key="key")
index = pinecone.Index("my-index")
index.upsert(vectors=[("id1", [0.1, 0.2, ...], {"key": "value"})])
```
**Pros:**
- Fully managed (no infra)
- Massive scale
- Real-time updates
- Great dashboard

**Cons:**
- Paid service
- Requires internet
- Data leaves your system

#### When to Use What?

| Scenario | Recommendation |
|----------|----------------|
| Learning/Prototyping | **ChromaDB** |
| Speed-critical research | **FAISS** |
| Production with budget | **Pinecone** or **Qdrant Cloud** |
| Self-hosted production | **Weaviate** or **Qdrant** |
| Existing Postgres | **pgvector** |

---

## ğŸ”„ 5. Complete RAG Pipeline

### What is RAG?
**R**etrieval-**A**ugmented **G**eneration = Give the LLM relevant context before asking it to answer.

### Why RAG?
| Problem | Without RAG | With RAG |
|---------|-------------|----------|
| LLM knowledge cutoff | "I don't know about 2024" | "Based on the 2024 10-K..." |
| Hallucination | Makes up facts | Cites actual documents |
| Specificity | Generic answers | Company-specific answers |
| Verifiability | "Trust me" | "Source: JPM 10-K page 47" |

### RAG Pipeline Architecture
```
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚         RAG PIPELINE            â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INDEXING (Offline - Done Once):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   10-K      â”‚ â†’â†’ â”‚   Chunk     â”‚ â†’â†’ â”‚  Embedding  â”‚ â†’â†’ â”‚  Vector     â”‚
â”‚   Documents â”‚    â”‚   (split)   â”‚    â”‚   Model     â”‚    â”‚   Database  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

QUERYING (Online - Per Question):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User      â”‚ â†’â†’ â”‚  Embedding  â”‚ â†’â†’ â”‚  Vector     â”‚ â†’â†’ â”‚   Top-K     â”‚
â”‚   Question  â”‚    â”‚   Model     â”‚    â”‚   Search    â”‚    â”‚   Chunks    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Answer    â”‚ â†â† â”‚  LLM (GPT-4/Gemini): "Based on the context:     â”‚
â”‚   + Source  â”‚    â”‚  JPM's main risks include..." [Source: ...]     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components

| Component | Our Implementation | Purpose |
|-----------|-------------------|---------|
| **Document Loader** | `A_SEC_EDGAR.py` | Download 10-K filings |
| **Chunker** | `B_Chunking_Indexing.py` | Split into 600-token chunks |
| **Embedding Model** | all-MiniLM-L6-v2 | Convert text â†’ 384D vectors |
| **Vector Store** | ChromaDB | Store and search vectors |
| **Retriever** | `C_Retrieval.py` | Find top-5 relevant chunks |
| **Generator** | `D_Generation.py` | LLM generates answer with citations |
| **API** | `api.py` (FastAPI) | REST API for queries |

### Our Numbers

| Metric | Value |
|--------|-------|
| Documents | 3 (JPM, GS, UBS 10-Ks) |
| Total Chunks | 2,332 |
| Chunk Size | 600 tokens (~450 words) |
| Vector Dimensions | 384 |
| Retrieval Time | ~38ms |
| Top-K Retrieved | 5 chunks per query |

---

## ğŸ”§ 6. Where is RAG Actually Coded? (Implementation Map)

### RAG = R (Retrieval) + A (Augmented) + G (Generation)

**The RAG core is in `D_Generation.py` in the `RAGEngine.query()` method.**

### Complete Implementation Map

| RAG Component | File | Lines | What It Does |
|---------------|------|-------|--------------|
| **R - Retrieval** | `C_Retrieval.py` | 100-150 | Semantic search in ChromaDB |
| **A - Augmentation** | `D_Generation.py` | 330-360 | Format chunks as LLM context |
| **G - Generation** | `D_Generation.py` | 400-420 | Call LLM with context |

### Visual Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    YOUR RAG PIPELINE                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ A_SEC_EDGAR.py        - Downloads 10-K documents         â”‚  â”‚
â”‚  â”‚ B_Chunking_Indexing.py - Chunks + Embeddings â†’ ChromaDB  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                         â†“ PREPROCESSING (done once)             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚  D_Generation.py (RAGEngine.query method)                â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚    â”‚ R:RETRIEVAL â”‚ â†’ â”‚ A:AUGMENT   â”‚ â†’ â”‚ G:GENERATE  â”‚  â”‚  â”‚
â”‚  â”‚    â”‚             â”‚   â”‚             â”‚   â”‚             â”‚  â”‚  â”‚
â”‚  â”‚    â”‚ Search      â”‚   â”‚ Format      â”‚   â”‚ Call LLM    â”‚  â”‚  â”‚
â”‚  â”‚    â”‚ ChromaDB    â”‚   â”‚ Context     â”‚   â”‚ Get Answer  â”‚  â”‚  â”‚
â”‚  â”‚    â”‚ for top-k   â”‚   â”‚ for LLM     â”‚   â”‚             â”‚  â”‚  â”‚
â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â”‚         â†‘                                                â”‚  â”‚
â”‚  â”‚    C_Retrieval.py                                        â”‚  â”‚
â”‚  â”‚    (Retriever class)                                     â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The Exact RAG Code (D_Generation.py)

```python
def query(self, question: str, k: int = 5, company: str = None) -> RAGResponse:
    """
    Execute a RAG query: retrieve context and generate answer.
    """
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STEP 1: RETRIEVAL (R) - Find relevant chunks
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    chunks = self.retriever.retrieve(
        query=question,
        k=k,
        company=company
    )
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STEP 2: AUGMENTATION (A) - Build context from chunks
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    context = self._format_context(chunks)  # Combine chunks into prompt
    user_prompt = self._create_user_prompt(question, context)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STEP 3: GENERATION (G) - LLM generates answer
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    answer = self.llm.generate(SYSTEM_PROMPT, user_prompt)
    
    return RAGResponse(answer=answer, citations=citations, ...)
```

### Key Code Sections

#### 1. RETRIEVAL - `C_Retrieval.py`
```python
def retrieve(self, query: str, k: int = 5) -> List[Dict]:
    # Convert query to embedding
    query_embedding = self.embedding_model.encode([query])
    
    # Search ChromaDB for similar chunks
    results = self.collection.query(
        query_embeddings=query_embedding,
        n_results=k
    )
    return results
```

#### 2. AUGMENTATION - `D_Generation.py`
```python
def _format_context(self, chunks: List[Dict]) -> str:
    """Combine retrieved chunks into context for LLM"""
    context_parts = []
    for chunk in chunks:
        context_parts.append(
            f"[Source: {chunk['company']} - {chunk['section']}]\n"
            f"{chunk['text']}"
        )
    return "\n---\n".join(context_parts)
```

#### 3. GENERATION - `D_Generation.py`
```python
# Generate answer using LLM with context
answer = self.llm.generate(
    system_prompt=SYSTEM_PROMPT,  # "Only answer from context..."
    user_prompt=f"CONTEXT:\n{context}\n\nQUESTION: {question}"
)
```

### Summary

**RAG is NOT a single function** - it's the **combination** of:

1. **Retrieval** â†’ Finding relevant documents (ChromaDB search in `C_Retrieval.py`)
2. **Augmentation** â†’ Adding those documents to the prompt (`D_Generation.py`)
3. **Generation** â†’ LLM answering with that context (`D_Generation.py`)

**The "magic" happens in `D_Generation.py`** in the `RAGEngine.query()` method where all three steps come together!

---

## ğŸ’¡ Interview Tips

### Common Questions & Answers

**Q: Why not just use the full document?**
> A: LLMs have context limits (GPT-4: 128K tokens). A 10-K can have 300K+ tokens. Also, retrieving specific chunks is more precise and cost-effective.

**Q: Why 600 tokens per chunk?**
> A: Sweet spot between context (enough info to be useful) and precision (specific enough to match queries). Industry standard is 500-800.

**Q: Why overlap chunks?**
> A: Sentences at chunk boundaries might get cut off. 100-token overlap ensures continuity.

**Q: ChromaDB vs FAISS?**
> A: ChromaDB for ease of use + metadata. FAISS for pure speed at massive scale. We prioritized developer experience.

**Q: How do you prevent hallucination?**
> A: 1) System prompt enforces "only answer from context", 2) Citations required, 3) Confidence scoring, 4) "I don't know" responses when context insufficient.

**Q: How would you scale this?**
> A: 1) Cloud vector DB (Pinecone/Qdrant), 2) Async processing, 3) Caching frequent queries, 4) Multiple retrieval strategies (hybrid search).

**Q: Where is RAG implemented in your code?**
> A: The main RAG logic is in `D_Generation.py` in the `RAGEngine.query()` method. It calls retrieval from `C_Retrieval.py`, formats the context, and sends it to the LLM.

---

## ğŸ“Š Quick Reference Card

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RAG QUICK REFERENCE                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  TOKEN: Smallest unit LLM processes (~0.75 words)               â”‚
â”‚  CHUNK: Document piece (~600 tokens) for retrieval              â”‚
â”‚  EMBEDDING: Text â†’ Vector (list of numbers)                     â”‚
â”‚  DIMENSION: Each number in vector (384 for our model)           â”‚
â”‚  VECTOR DB: Database optimized for similarity search            â”‚
â”‚  SIMILARITY: How "close" two vectors are (0-1 scale)            â”‚
â”‚  TOP-K: Number of chunks to retrieve (we use 5)                 â”‚
â”‚  RAG: Retrieval-Augmented Generation                            â”‚
â”‚                                                                 â”‚
â”‚  FORMULA:                                                       â”‚
â”‚  Query â†’ Embed â†’ Search VectorDB â†’ Get Chunks â†’ LLM â†’ Answer    â”‚
â”‚                                                                 â”‚
â”‚  CODE LOCATION:                                                 â”‚
â”‚  D_Generation.py â†’ RAGEngine.query() â†’ The RAG magic!           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

*Last updated: January 2026*
*Project: SEC EDGAR 10-K RAG Pipeline*
